{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIST MGE303 Data Mining\n",
    "## Lab Session 01 | 2020-04-20 (MON)\n",
    "### Seok-Ju Hahn (sjhahn11512@unist.ac.kr)\n",
    "\n",
    "\n",
    "\n",
    "## Supervised Learning (Regression): House Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "- Load packages using `import` command and alias command (`as`)\n",
    "- Remember trio: numpy, pandas, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Munging\n",
    "* <a href='#Load-data'>Load data</a>\n",
    "* <a href='#Handle-data'>Handle data</a>\n",
    "* <a href='#Split-training-and-test-data'>Split training and test data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "* Load data using `read_csv()` method\n",
    "  - Pandas package read data as `DataFrame` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check the first few rows of data using `head()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle data\n",
    "* Get some parts of data using `iloc[row_index, column_index]` and `loc[row_index_name, column_name]` methods\n",
    "  - `:` means 'all'\n",
    "  - Attaching `.values` returns numpy array, if not it reutrns `pd.Series` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Get the first column of data\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Get the first column of data attaching `.values`\n",
    "= CAUTION: in Python, index strats at 0\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Get the 11th data of 'total_rooms' feature \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can refer a specific column using `[]` notation or `.column_name` notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Get the 'latitude' column\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check simple description of data using `info()` method\n",
    "  - Column name\n",
    "  - Column/Row counts\n",
    "  - Data types\n",
    "  - Counts of non-null samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check a categorical feature using `value_counts()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check summarized information of numerical features using `describe()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot histogram of each column to check sanity of the data\n",
    "  - Attach `hist()` method to your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What things can you find in the above histograms?\n",
    "- 'median_income(중위 소득)' variable is not represented in dollar. (See horizontal axis)\n",
    "- Maximum values of 'housing_median_age(중위 주택 연도)' and 'median_house_value(중위 주택 가격)' variables are strange.\n",
    "  - It is intentionally set to be limited by data collector.\n",
    "  - So, you don't have to care, but for other dataset, you should detect and ask this kinds of facts to data engineers.\n",
    "- Scales (value range) of predictors are different from each other.\n",
    "  - It is to be handled in data pre-processing stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training and test data\n",
    "- Remember, you **MUST** split test data first for simulating unseen data\n",
    "- Use `train_test_split()` method in scikit-learn package\n",
    "- If you use test set on training your models, it underestimates generalization error, which induces __data snooping bias__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import method\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `random_state` for reproduciblity\n",
    "training_set, test_set = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check sample counts of training and test set using `len()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_LENGTH = \n",
    "TE_LENGTH = \n",
    "\n",
    "print(f'Training samples: {TR_LENGTH}, Test samples: {TE_LENGTH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data for preventing damage in raw training data\n",
    "data = training_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot scatter-plot of your training data for indicating house location\n",
    "  - Attach `plot(kind='scatter', x='', y='')` method to your dataframe\n",
    "  - You can add `alpha` argument for effective representation of scatter density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = \n",
    "ax.set(xlabel='Longitude', ylabel='Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate Pearson's r correlation coefficient among predictors\n",
    "  - Attach `corr()` method to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = \n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inspect which variable is highly correlated with house value\n",
    "  - Attach `sort_values(ascending=False)` method to correlation matrix you made, after referring the 'median_house_value' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seems like 'median_income' is highly correalted with house price\n",
    "  - let us see scatter plot again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yes, it seems better to remove the uppermost lines (due to intentional limit by data engineers).\n",
    "* You can filter out samples to be remained by providing conditions in `[]` notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can do more!\n",
    "  - <u>Create new features</u> by combining existing predictors\n",
    "  - This kind of work can strengthen our hypothesis (if it is done in a sophisticated manner) \n",
    "\n",
    "* For example, it is better to know the number of rooms per household rather than 'total_rooms'.\n",
    "* Same logic can be applied for 'total_bedrooms' and 'population'\n",
    "  - Let us make new features: 'rooms_per_household', 'bedrooms_per_room', 'people_per_household'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rooms_per_household'] = data['total_rooms'] / data['households']\n",
    "data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']\n",
    "data['people_per_household'] = data['population'] / data['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation matrix again for additional features\n",
    "correlation_matrix = data.corr()\n",
    "correlation_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "* <a href='#Cleanse-data'>Cleanse data</a>\n",
    "* <a href='#Scale-data'>Scale data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanse data\n",
    "* Handle missing values\n",
    "  - Just remove rows with missing values\n",
    "  - Impute missing values using mean, median, or imputation algorithms (NOT covered today)\n",
    "  - Collect data again\n",
    "* Drop unncessary columns\n",
    "* Remove duplicated samples\n",
    "* Convert categorical data into numerical representation (encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle missing values\n",
    "- Remove rows with missing value\n",
    "- Attach `dropna()` to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have missing data in 'total_bedrooms' and 'bedrooms_per_room' features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop unncessary columns\n",
    "- As we made new features ('rooms_per_household', 'bedrooms_per_room', 'people_per_household'), let us remove features used for making three predictors\n",
    "- Attach `drop(columns=['COLUMN_NAME'], axis=1)` to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['total_rooms', 'total_bedrooms', 'population', 'households'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop duplicated samples\n",
    "- Attach `drop_duplicates()` to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Advanced) Automation - <a href='#AddFeatures'>AddFeatures</a>\n",
    "- This process can be automated by constructing a simple function\n",
    "- Automation of such a process is important since data mining process requires fast prototyping and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encode cateogrical feature\n",
    "- Convert categorical feature represented in string format into numerical representation ('ocean_proximity' feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us first separate numerical and categorical columns\n",
    "cat_feat = ['ocean_proximity']\n",
    "num_feat = ['longitude', 'latitude', 'housing_median_age', 'median_income', 'rooms_per_household', 'bedrooms_per_room', 'people_per_household']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Beware* that we at first need to split out the dependent variable first!\n",
    "- Use `drop()` and `loc()` method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `OneHotEncoer` provided by Scikit-Learn package\n",
    "- Select 'ocean_proximity' column only using `[]` or `.COLUMN_NAME` or `loc()` or `iloc()`, and call `fit_transform()` method to `OneHotEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(data['ocean_proximity'].values.reshape(-1, 1))\n",
    "X_train_cat = encoder.transform(data['ocean_proximity'].values.reshape(-1, 1))\n",
    "\n",
    "# OR\n",
    "# X_train_cat = encoder.fit_transform(data['ocean_proximity'].values.reshape(-1, 1))\n",
    "\n",
    "# CAUTION! for test set, you need to fit on training data first, and SHOULD only transform test set!\n",
    "\"\"\"\n",
    "example code snippet)\n",
    "  encoder.fit(training_data)\n",
    "  training_data = encoder.transform(training_data)\n",
    "  test_data = encoder.transform(test_data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, all data are transformed into numerical values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale data\n",
    "* Feature scaling means to transform ranges of all **numerical** features to be similar with each other. <br> (it is enough to just one-hot-encode categorical features)\n",
    "* Standard scaling (standardization) is to make feature to have mean 0 and standard deviation 1.\n",
    "  - It is **TOTALLY different** from converting data distribution to Gaussian ditsribution!!!\n",
    "  - Except for models having assumption of Gaussian distributed data, such as Linear Discriminant analysis, Gaussian Mixture models, <br>\n",
    "  it is NOT needed to convert data distribution to be Gaussian.\n",
    "  - It is just shift the range of feature distribution\n",
    "* Feature scaling is especially important for algorithms:\n",
    "  - based on Euclidean distance like K-means clustering, k-NN (different scale distorts distance measure)\n",
    "  - based on gradient-based optimizations like logistic regression, neural networks (different scales distorts loss surface)\n",
    "  - regard scale of features with significance like PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale numerical features\n",
    "- Scale numerical features to have mean 0 and standard deviation 1\n",
    "- Use `StandardScaler` provided by Scikit-Learn package\n",
    "- Select numerical feature columns and call `fit_trnasform()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar = StandardScaler()\n",
    "X_train_num = scalar.fit_transform(X_train[num_feat].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finish up data pre-processing\n",
    "- Now, we need to concatenate categorical (one-hot encoded) and numerical (standardized) features!\n",
    "- It can be easily done by `np.concatenate([*arrays], axis=1)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = \n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget! You should do the same process above on the test set you made in <a href='#Split-training-and-test-data'>here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Advanced) Automation - <a href='#Pipeline'>Pipeline</a>\n",
    "- This process can be also automated by using `Pipeline` and `ColumnTransformer` method \n",
    "- Automation of such a process is important since data mining process requires fast prototyping and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and evaluation\n",
    "* <a href='#Train-model'>Train model</a>\n",
    "* <a href='#Evaluate-model'>Evaluate model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "* Choose an appropriate algorithm for your problem setting\n",
    "* There are tons of ready-made algorithms in here: <a href='https://scikit-learn.org/stable/supervised_learning.html'>Scikit-Learn</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To train model, we need to create model instance such as `LinearRegression()` and call `fit()` method by providing independent and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = \n",
    "lin_reg.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Performance of the trained model on the training set can easily be found by calling `score()` method (which returns r-squared) or calling another metrics like `mean_squared_error` with result from `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "y_pred_train = lin_reg.predict()\n",
    "\n",
    "lin_reg_mse = mean_squared_error()\n",
    "lin_reg_mae = mean_absolute_error()\n",
    "\n",
    "print(f'MAE: {lin_reg_mae:.4f}, MSE: {lin_reg_mse:.4f}, RMSE: {np.sqrt(lin_reg_mse):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model\n",
    "* Internal evaluation\n",
    "  - Evaluate the performance of the trained model using training data by simulating training-test split internally.\n",
    "  - Bootstrapping (NOT covered), cross-validation\n",
    "* External evaluation\n",
    "  - Evaluate the performance of the trained model using unseen data (test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Internal evaluation\n",
    "- Can be easily done by `cross_val_score()` method\n",
    "- When passing argument `cv=10`, it executes 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(LinearRegression(), X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "lin_reg_rmse_cv_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scores: {lin_reg_rmse_cv_scores},\\nMean: {np.mean(lin_reg_rmse_cv_scores):.4f},\\nStd: {np.std(lin_reg_rmse_cv_scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### External evaluation\n",
    "- After processing <a href='#Split-training-and-test-data'>test set</a> you split above in the same way as training set, measure the performance of the trained model on this test set\n",
    "- Use `predict()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lin_reg.predict()\n",
    "\n",
    "lin_reg_mse = mean_squared_error()\n",
    "lin_reg_mae = mean_absolute_error()\n",
    "\n",
    "print(f'MAE: {lin_reg_mae:.4f}, MSE: {lin_reg_mse:.4f}, RMSE: {np.sqrt(lin_reg_mse):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Advanced) Automation\n",
    "- Full process can be done by automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AddFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddFeatures():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        X['rooms_per_household'] = X['total_rooms'] / X['households']\n",
    "        X['bedrooms_per_room'] = X['total_bedrooms'] / X['total_rooms']\n",
    "        X['people_per_household'] = X['population'] / X['households']\n",
    "        \n",
    "        X.drop(columns=['total_rooms', 'total_bedrooms', 'population', 'households'], axis=1, inplace=True)\n",
    "        \n",
    "        return X.dropna(), y[y.index.isin(X.dropna().index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# for numerical features\n",
    "num_pipeline = Pipeline([('standardization', StandardScaler())])\n",
    "\n",
    "# for categorical features\n",
    "cat_pipeline = Pipeline([('one_hot_encoding', OneHotEncoder(sparse=False))])\n",
    "\n",
    "#X_train_num = num_pipeline.fit_transform(X_train[num_feat])\n",
    "#X_train_cat = = cat_pipeline.fit_transform(X_train[cat_feat])\n",
    "#X_train = np.concatenate([X_train_num, X_train_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cat_feat = ['ocean_proximity']\n",
    "num_feat = ['longitude', 'latitude', 'housing_median_age', 'median_income']\n",
    "\n",
    "pipelines = ColumnTransformer([('numeric_features', num_pipeline, num_feat), ('categorical_feature', cat_pipeline, cat_feat)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preapare and split data\n",
    "X_train, y_train = AddFeatures().transform(training_set.drop(columns=['median_house_value'], axis=1, inplace=False), training_set.loc[:, ['median_house_value']])\n",
    "X_test, y_test = AddFeatures().transform(test_set.drop(columns=['median_house_value'], axis=1, inplace=False), test_set.loc[:, ['median_house_value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test set, we need to call `transform`, because we don't know the test set in prior\n",
    "X_train = pipelines.fit_transform(X_train)\n",
    "X_test = pipelines.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([('pre_processing', pipelines), ('linear_regression', LinearRegression())])\n",
    "\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "cv_score = np.sqrt(-cross_val_score(full_pipeline, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))\n",
    "test_prediction = full_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
